{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1596b77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0,str(Path(Path(os.getcwd()).parent, \"3rdparty/rulecosi\")))\n",
    "import rulecosi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57f16f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tree_diff\n",
    "# import importlib\n",
    "# importlib.reload(tree_diff.tree_similarity)\n",
    "# importlib.reload(tree_diff.stages.baseline_models)\n",
    "from tree_diff.tree_similarity import rule_set_similarity\n",
    "from tree_diff.stages.baseline_models import oneHotToDense, oneHot\n",
    "from tree_diff.training_utils import report_metrics, SCORING, rule_overlap, rule_sparsity, train_model\n",
    "from tree_diff.rule_entities import *\n",
    "import tree_diff.config\n",
    "config = tree_diff.config.Config()\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score, f1_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "import imodels\n",
    "\n",
    "from river import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6964cec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = [\"age\",\"workclass\",\"fnlwgt\",\"education\",\"education-num\",\"marital\",\"occupation\",\"relationship\",\"race\",\"sex\",\"capital-gain\",\"capital-loss\",\"hours\",\"native\"]\n",
    "Y_COLUMN = \"income\"\n",
    "STRING_COLUMNS = [\"workclass\",\"education\",\"marital\",\"occupation\",\"relationship\",\"race\",\"sex\",\"native\",]\n",
    "\n",
    "def load_batch(file_path, columns=FEATURES+[Y_COLUMN]):\n",
    "    file_path = Path(Path().absolute().parent, \"input/adult/\", file_path)\n",
    "    batch = pd.read_csv(file_path, header=None)\n",
    "    batch.columns = columns\n",
    "    return batch\n",
    "    \n",
    "batch_1_train = load_batch(\"batch_1/adult1.data\")\n",
    "batch_1_test = load_batch(\"batch_1/adult1.test\")\n",
    "\n",
    "batch_2_train = load_batch(\"batch_2/adult2.data\")\n",
    "batch_2_test = load_batch(\"batch_2/adult2.test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a47316",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer1 = tree_diff.training_utils.train_model(rulecosi.RuleCOSIClassifier(base_ensemble=GradientBoostingClassifier()),\n",
    "                                    batch_1_train[FEATURES],\n",
    "                                    batch_1_train[Y_COLUMN],                                     \n",
    "                                    oneHotToDense(STRING_COLUMNS), \n",
    "                                    config, \n",
    "                                    \"rulecosi\")\n",
    "\n",
    "scorer2 = tree_diff.training_utils.train_model(rulecosi.RuleCOSIClassifier(base_ensemble=GradientBoostingClassifier()),\n",
    "                                    batch_2_train[FEATURES],\n",
    "                                    batch_2_train[Y_COLUMN],                                     \n",
    "                                    oneHotToDense(STRING_COLUMNS), \n",
    "                                    config, \n",
    "                                    \"rulecosi\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3f3c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rulecosi_rule_to_conditions(rule):    \n",
    "    conditions = [Condition(a[1].att_name, \n",
    "                            tree_diff.rule_entities.import_operator(a[1].op.__name__), a[1].value) \n",
    "                  for a in rule.A]\n",
    "    return Rule(label = rule.y, conditions=conditions)\n",
    "\n",
    "def rulecosi_rules_to_converted_rules(results, key):\n",
    "    rulecosi_rules = results['estimator'][-1][key].simplified_ruleset_.rules\n",
    "    converted_rules = list(map(rulecosi_rule_to_conditions, rulecosi_rules))\n",
    "    return Ruleset(rules = converted_rules)\n",
    "\n",
    "RS1 = rulecosi_rules_to_converted_rules(scorer1, \"rulecosi\")\n",
    "RS2 = rulecosi_rules_to_converted_rules(scorer2, \"rulecosi\")\n",
    "print(report_metrics(scorer1, SCORING))\n",
    "print(report_metrics(scorer2, SCORING))\n",
    "print(f\"RS1 overlap {rule_overlap(RS1):0.2f}, RS1 sparsity {rule_sparsity(RS1):0.2f}\")\n",
    "print(f\"RS2 overlap {rule_overlap(RS2):0.2f}, RS2 sparsity {rule_sparsity(RS2):0.2f}\")\n",
    "print(f\"Similarity: {rule_set_similarity(RS1, RS2):0.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75849c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer3 = tree_diff.training_utils.train_model(rulecosi.RuleCOSIClassifier(base_ensemble=GradientBoostingClassifier()),\n",
    "                                    pd.concat([batch_1_train[FEATURES], batch_2_train[FEATURES]]),\n",
    "                                    pd.concat([batch_1_train[Y_COLUMN], batch_2_train[Y_COLUMN]]),                                     \n",
    "                                    oneHotToDense(STRING_COLUMNS), \n",
    "                                    config, \n",
    "                                    \"rulecosi\")\n",
    "RS3 = rulecosi_rules_to_converted_rules(scorer3, \"rulecosi\")\n",
    "print(report_metrics(scorer3, SCORING))\n",
    "print(f\"RS3 overlap {rule_overlap(RS3):0.2f}, RS3 sparsity {rule_sparsity(RS3):0.2f}\")\n",
    "print(f\"Similarity: {rule_set_similarity(RS1, RS3):0.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2512df18",
   "metadata": {},
   "outputs": [],
   "source": [
    "scap1 = tree_diff.training_utils.train_model(GradientBoostingClassifier(),\n",
    "                                    batch_1_train[FEATURES],\n",
    "                                    batch_1_train[Y_COLUMN],                                     \n",
    "                                    oneHotToDense(STRING_COLUMNS), \n",
    "                                    config, \n",
    "                                    \"scap\")\n",
    "\n",
    "scap2 = tree_diff.training_utils.train_model(GradientBoostingClassifier(),\n",
    "                                    batch_2_train[FEATURES],\n",
    "                                    batch_2_train[Y_COLUMN],                                     \n",
    "                                    oneHotToDense(STRING_COLUMNS), \n",
    "                                    config, \n",
    "                                    \"scap\")\n",
    "print(report_metrics(scap1, SCORING))\n",
    "print(report_metrics(scap2, SCORING))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9be6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ruleset(X, y):\n",
    "    rulecosi_plus = tree_diff.training_utils.train_model(rulecosi.RuleCOSIClassifier(base_ensemble=GradientBoostingClassifier()),\n",
    "                                        X,\n",
    "                                        y,                                     \n",
    "                                        oneHotToDense(STRING_COLUMNS), \n",
    "                                        config, \n",
    "                                        \"rulecosi\")\n",
    "    print(report_metrics(rulecosi_plus, SCORING))\n",
    "    return rulecosi_rules_to_converted_rules(rulecosi_plus, \"rulecosi\"), rulecosi_plus['estimator'][-1]\n",
    "\n",
    "# Input: Ruleset, Dataset\n",
    "\n",
    "D_2_X = pd.concat([batch_1_train[FEATURES], batch_2_train[FEATURES]])\n",
    "D_2_y = pd.concat([batch_1_train[Y_COLUMN], batch_2_train[Y_COLUMN]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce82f026",
   "metadata": {},
   "source": [
    "Training on the subset will ensure there is almost 0 overlap. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4ed11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "RS_all, RS_all_model = train_ruleset(D_2_X, D_2_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb58b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_new = RS_all_model.predict(D_2_X)\n",
    "\n",
    "new_indexes = [i for i in indexes if y_pred_new[i] != D_2_y.iloc[i]]\n",
    "len(indexes), len(new_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01168925",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "def report_metrics(scores, scoring):\n",
    "    strings = []\n",
    "    for k in scoring.keys():\n",
    "        key = f\"test_{k}\"\n",
    "        s = f\"{scores[key].mean():0.2f}\"\n",
    "        strings.append(f\"{k}: {s}\")\n",
    "    return \", \".join(strings)\n",
    "\n",
    "def var_name(var):\n",
    "    vars = [v for v in globals() if globals()[v] == var]\n",
    "    return vars[0] if len(vars) > 0 else None\n",
    "\n",
    "def train_model(model, X, y, preprocessor, config, model_name=None):\n",
    "    pipeline = []\n",
    "    if preprocessor:\n",
    "        for p in preprocessor:\n",
    "            pipeline.append(p)\n",
    "    pipeline.append((model_name if model_name else str(model), model))\n",
    "    pipeline = Pipeline(pipeline)\n",
    "    scores = cross_validate(pipeline, X, y, return_estimator=True, cv=config.cv, scoring=SCORING)\n",
    "    #LOGGER.info(f\"{str(model)} : {report_metrics(scores, SCORING)}\")\n",
    "    return scores\n",
    "\n",
    "figs1 = train_model(imodels.FIGSClassifier(), \n",
    "            batch_1_train[FEATURES],\n",
    "            [1 if i == ' <=50K' else 0 for i in  list(batch_1_train[Y_COLUMN])],\n",
    "            oneHotToDense(STRING_COLUMNS), \n",
    "            config,\n",
    "            \"FIGS\")\n",
    "\n",
    "figs2 = train_model(imodels.FIGSClassifier(), \n",
    "            D_2_X,\n",
    "            [1 if i == ' <=50K' else 0 for i in  list(D_2_y)],\n",
    "            oneHotToDense(STRING_COLUMNS), \n",
    "            config,\n",
    "            \"FIGS\")\n",
    "print(report_metrics(figs1, SCORING))\n",
    "print(report_metrics(figs2, SCORING))\n",
    "\n",
    "# Evidence that Decision Sets are more interpretable than Decision Lists: IDS paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277efd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "figs1['estimator'][-1][-1].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15163f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "figs1['estimator'][-1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4925e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_tree(node, fetch_children, is_leaf):\n",
    "    \"\"\"Generator function that walks a tree.\n",
    "\n",
    "    fetch_children: Callable that accepts a node and path_to_node.\n",
    "        Returns a List[Tuple] where Tuple = (path_to_node, child_node)\n",
    "\n",
    "    is_leaf: Callable that accepts a node and returns true if a leaf node.\n",
    "\n",
    "    Usage:\n",
    "    >>> [p for p in walk_tree(tree, lambda x, p: [(p, x.left), (p, x.right)], lambda x: x.is_leaf)]\n",
    "    \"\"\"\n",
    "    stack = deque()\n",
    "    stack.append(([],node))\n",
    "    while stack:\n",
    "        path_to_node, node = stack.pop()\n",
    "        if is_leaf(node):\n",
    "            yield path_to_node + [node]\n",
    "        else:\n",
    "            children = fetch_children(node, path_to_node)\n",
    "            for child in children:\n",
    "                stack.append(child)\n",
    "\n",
    "def children(node, path):\n",
    "    return [(path +[node], node.left), (path +[node], node.right)]\n",
    "              \n",
    "def is_leaf(node):\n",
    "    return node.left is None and node.right is None\n",
    "\n",
    "def create_conditions(path_conds):\n",
    "    return [Condition(f\"attr_{c.feature}\", Operator.LE, c.threshold) for c in path_conds]\n",
    "\n",
    "def create_rule(path):\n",
    "    return Rule(conditions=create_conditions(path[0:-1]), label=f\"{path[-1].value[0][0]:0.2f}\")\n",
    "\n",
    "def extract_rules(tree):\n",
    "    return [create_rule(p) for p in walk_tree(tree, children, is_leaf)]\n",
    "\n",
    "RS_FIG1 = Ruleset(rules=[i for t in figs1['estimator'][-1][-1].trees_ for i in extract_rules(t)])\n",
    "RS_FIG2 = Ruleset(rules=[i for t in figs2['estimator'][-1][-1].trees_ for i in extract_rules(t)])\n",
    "\n",
    "print(f\"RS_FIG1 overlap {rule_overlap(RS_FIG1):0.2f}, RS_FIG1 sparsity {rule_sparsity(RS_FIG1):0.2f}\")\n",
    "print(f\"RS_FIG2 overlap {rule_overlap(RS_FIG2):0.2f}, RS_FIG2 sparsity {rule_sparsity(RS_FIG2):0.2f}\")\n",
    "print(f\"Similarity: {rule_set_similarity(RS_FIG1, RS_FIG2):0.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a3dcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "RS_FIG1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badf22e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "RS_FIG2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db3cfc2",
   "metadata": {},
   "source": [
    "# Algorithm Notes\n",
    "Algorithms contain the following steps:\n",
    "* Rule candidate generation\n",
    "* Rule selection \n",
    "* Rule post-processing\n",
    "\n",
    "Decision sets should be the focus as they minimise the barriers to interpretability:\n",
    "* New rules can be added or removed without affecting the rest of the model (locally modifiable)\n",
    "* Changes to existing rules does not require reasoning about the rest of the model (locally verifiable)\n",
    "* Decision sets are globally and locally interpretable (which rules fired, and what logic is encoded)\n",
    "\n",
    "Other observations\n",
    "* Output is a decision tree (Decision sets will be left to the extension) \n",
    "* Need a new similarity measure that supports similarity of decision sets (sort by attributes by name, operator and threshold).\n",
    "\n",
    "TODO:\n",
    "* Train on Extremely Fast Decison Tree\n",
    "* Demonstrate idea with pretrained model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb84c7d9",
   "metadata": {},
   "source": [
    "# Simplified Problem Setting\n",
    "\n",
    "* Assume a tree model has been pre-trained on D1, T_original\n",
    "* Assume a tree ensemble has been trained on data D1 + D2 -> E = {T_1, T_2 ... T_n}\n",
    "\n",
    "How can we extend the decision tree T_original with the fewest modifications to achieve the same accuracy as the tree ensemble, E?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed85cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "T_original = tree_diff.training_utils.train_model(DecisionTreeClassifier(),\n",
    "                                    batch_1_train[FEATURES],\n",
    "                                    batch_1_train[Y_COLUMN],                                     \n",
    "                                    oneHotToDense(STRING_COLUMNS), \n",
    "                                    config, \n",
    "                                    \"T_original\")\n",
    "\n",
    "D_2_X = pd.concat([batch_1_train[FEATURES], batch_2_train[FEATURES]])\n",
    "D_2_y = pd.concat([batch_1_train[Y_COLUMN], batch_2_train[Y_COLUMN]])\n",
    "\n",
    "E = tree_diff.training_utils.train_model(DecisionTreeClassifier(),\n",
    "                                    D_2_X,\n",
    "                                    D_2_y,                                     \n",
    "                                    oneHotToDense(STRING_COLUMNS), \n",
    "                                    config, \n",
    "                                    \"E\")\n",
    "\n",
    "print(report_metrics(T_original, SCORING))\n",
    "print(report_metrics(E, SCORING))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c4f788",
   "metadata": {},
   "source": [
    "### Baseline:\n",
    "Using an incremental learning decision tree as the base line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d119e6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tree.ExtremelyFastDecisionTreeClassifier(max_depth = 1)\n",
    "\n",
    "def test_incrementally(model, X_df, y_df):\n",
    "    y_pred = []\n",
    "    for i in range(0, len(X_df)):\n",
    "        x = dict(X_df.iloc[i])\n",
    "        y_pred.append(model.predict_one(x))\n",
    "    return f1_score([1 if i == ' <=50K' else 0 for i in y_df], \n",
    "                    [1 if i == ' <=50K' else 0 for i in y_pred])  \n",
    "\n",
    "def train_incrementally(model, X_df, y_df):\n",
    "    for i in range(0, len(X_df)):\n",
    "        x = dict(X_df.iloc[i])\n",
    "        y = y_df.iloc[i]    \n",
    "        model.learn_one(x,y)\n",
    "\n",
    "train_incrementally(model, batch_1_train[FEATURES], batch_1_train[Y_COLUMN])\n",
    "test_incrementally(model, batch_1_test[FEATURES], batch_1_test[Y_COLUMN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02035179",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = copy.deepcopy(model)\n",
    "train_incrementally(model2, batch_2_train[FEATURES], batch_2_train[Y_COLUMN])\n",
    "test_incrementally(model2, batch_2_test[FEATURES], batch_2_test[Y_COLUMN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8319413",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_incrementally(model, batch_2_test[FEATURES], batch_2_test[Y_COLUMN])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40555958",
   "metadata": {},
   "source": [
    "Need to convert incrementally trained trees to our rule set classes for similarity calculations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229995e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import river\n",
    "\n",
    "def river_children(node, path):\n",
    "    if isinstance(node, river.tree.nodes.efdtc_nodes.EFDTNominalMultiwayBranch):\n",
    "        return [(path + [(node, i)], n) for i, n in enumerate(node.children)]            \n",
    "    return [(path + [node], n) for n in node.children]\n",
    "              \n",
    "def river_is_leaf(node):\n",
    "    return node.n_leaves == 1\n",
    "\n",
    "def river_return_condition(node):\n",
    "    if isinstance(node, river.tree.nodes.efdtc_nodes.EFDTNumericBinaryBranch):\n",
    "        return Condition(f\"attr_{node.feature}\", Operator.LE, node.threshold)\n",
    "    elif isinstance(node, tuple):   # Multinomial \n",
    "        feature = node[0].feature\n",
    "        threshold = node[0]._r_mapping[node[1]]\n",
    "        return Condition(f\"attr_{feature}\", Operator.EQ, threshold)\n",
    "    else:\n",
    "        raise ValueError(node)\n",
    "\n",
    "def river_create_conditions(path_conds):\n",
    "    return [river_return_condition(c) for c in path_conds]\n",
    "\n",
    "def river_create_rule(path):\n",
    "    a = path[-1].stats\n",
    "    m = (None, 0)\n",
    "    for k, v in a.items():\n",
    "        if not m or m[1] < v:\n",
    "            m = (k,v)\n",
    "    label = m[0]            \n",
    "    return Rule(conditions=river_create_conditions(path[0:-1]), label=f\"{label}\")\n",
    "\n",
    "def river_extract_rules(tree, children, is_leaf):\n",
    "    return [river_create_rule(p) for p in walk_tree(tree, children, is_leaf)]\n",
    "\n",
    "rules_model1 = river_extract_rules(model._root,river_children, river_is_leaf)\n",
    "rules_model2 = river_extract_rules(model2._root,river_children, river_is_leaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ecff4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"EFDT Similarity: {rule_set_similarity(rules_model1, rules_model2):0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07aceb00",
   "metadata": {},
   "source": [
    "The similarity between two instances of the EFDT algorithm indicates the two models are quite close. This is to be expected as EFDT is a) an incremental learning algorithm designed to incrementally be adapated by concentrating on when to split, and b) allows for backtracking over the nodes to re-evaluate split decisions. \n",
    "\n",
    "**Recommendation**\n",
    "* Shift back to the rule sets or provide an additional complexity constraint to model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85055465",
   "metadata": {},
   "source": [
    "Convert Sklearn Tree to a decision set for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676a8ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rules_model2), len(RS2), len(RS_FIG2)\n",
    "# Tree, Rule List, Rule Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1266d2",
   "metadata": {},
   "source": [
    "# Sklearn Tree to rule set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16729037",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_tree = T_original['estimator'][-1]['T_original'].tree_\n",
    "\n",
    "def sklearn_children(node, path):\n",
    "    pass\n",
    "\n",
    "def sklearn_is_leaf(node):\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "# for p in walk_tree(sklearn_tree, sklearn_children, sklearn_is_leaf):\n",
    "#     print p\n",
    "\n",
    "river_extract_rules(model._root,river_children, river_is_leaf)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028c5923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "collections.Counter(batch_1_train[Y_COLUMN])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a624501f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred = \n",
    "y_pred = [not i for i in list(batch_1_train['capital-gain'] > 9090)]\n",
    "y_true = [i == ' <=50K' for i in list(batch_1_train[Y_COLUMN])]\n",
    "f1_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644430f6",
   "metadata": {},
   "source": [
    "# Android melware dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f18758b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://ieeexplore-ieee-org.ezproxy-b.deakin.edu.au/document/9312053\n",
    "    \n",
    "\n",
    "df_span = pd.read_csv(\"/Users/scott/projects/research-projects/tree_diff/input/TUANDROMD/TUANDROMD.csv\")\n",
    "\n",
    "LABEL = 'Label'\n",
    "FEATURES = [l for l in list(df_span.columns) if not l == LABEL] \n",
    "\n",
    "df_span.dropna(inplace=True)\n",
    "\n",
    "android_train_X, android_test_X, android_train_y, android_test_y = sklearn.model_selection.train_test_split(df_span[FEATURES],df_span[LABEL], test_size=0.3)\n",
    "\n",
    "android_train_y = [i == \"malware\" for i in android_train_y]\n",
    "android_test_y = [i == \"malware\" for i in android_test_y]\n",
    "    \n",
    "\n",
    "df_mushrooms = pd.read_csv(\"/Users/scott/projects/research-projects/tree_diff/input/MushroomDataset/secondary_data.csv\", delimiter=\";\")\n",
    "\n",
    "def classification_csv_loader(file_name, prediction_column, categorical_columns=None, **kwargs):\n",
    "    df = pd.read_csv(file_name, **kwargs)\n",
    "    if categorical_columns:\n",
    "        new_dfs = [pd.get_dummies(df[i], prefix=i, dummy_na=True) for i in categorical_columns]\n",
    "        df = pd.concat(new_dfs + [df[df.columns.difference(categorical_columns)]], axis=1)        \n",
    "    features = [l.strip() for l in list(df.columns) if not l == prediction_column]\n",
    "    feature_key = {v:i for i,v in enumerate(set(df[prediction_column]))}\n",
    "    df[prediction_column].replace(feature_key, inplace=True)\n",
    "    train_X, test_X, train_y, test_y = sklearn.model_selection.train_test_split(df[features],df[prediction_column], test_size=0.3)\n",
    "    return train_X, test_X, train_y.to_numpy(), test_y.to_numpy()\n",
    "\n",
    "\n",
    "categorical = ['cap-shape', 'cap-surface', 'cap-color',\n",
    "       'does-bruise-or-bleed', 'gill-attachment', 'gill-spacing', 'gill-color',\n",
    "       'stem-root', 'stem-surface', 'stem-color',\n",
    "       'veil-type', 'veil-color', 'has-ring', 'ring-type', 'spore-print-color',\n",
    "       'habitat', 'season']\n",
    "\n",
    "m_train_X, m_test_X, m_train_y, m_test_y = classification_csv_loader(\"/Users/scott/projects/research-projects/tree_diff/input/MushroomDataset/secondary_data.csv\", \n",
    "                            \"class\",\n",
    "                           categorical,\n",
    "                            delimiter=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a8419e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch(trained_model, X_df):\n",
    "    y_pred = []\n",
    "    if hasattr(trained_model, \"predict_one\"):\n",
    "        for i in range(0, len(X_df)):\n",
    "            x = dict(X_df.iloc[i])            \n",
    "            value = trained_model.predict_one(x)\n",
    "            y_pred.append(value)\n",
    "    else:\n",
    "        y_pred = trained_model.predict(X_df)\n",
    "    return y_pred\n",
    "\n",
    "def test_incrementally(trained_model, X_df, y_df):\n",
    "    assert len(X_df) == len(y_df), \"Instances and labels should be the same length\"\n",
    "    y_pred = predict_batch(trained_model, X_df)\n",
    "    assert len(y_pred) == len(y_df), \"Predition length should be same as labels\"\n",
    "    return balanced_accuracy_score(y_df, y_pred) \n",
    "\n",
    "# SGT are provided in the river project\n",
    "\n",
    "def train_incrementally(model_to_train, X_df, y_df):    \n",
    "    # Support incremental learning algorithms from river\n",
    "    if hasattr(model_to_train, \"learn_one\"):\n",
    "        for i in range(0, len(X_df)):\n",
    "            x = dict(X_df.iloc[i])\n",
    "            model_to_train.learn_one(x, y_df[i])\n",
    "    else:\n",
    "        model_to_train.fit(X_df, y_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "db27d53e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9642691415313225"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tree.ExtremelyFastDecisionTreeClassifier(max_depth=10)        \n",
    "train_incrementally(model, android_train_X, android_train_y)\n",
    "test_incrementally(model, android_test_X, android_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "f2b383d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9673106779368639"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(max_depth=5)\n",
    "train_incrementally(dt, android_train_X, android_train_y)\n",
    "test_incrementally(dt, android_test_X, android_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "7e3e2c3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7448184536832723, 20, 5)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(max_depth=5)\n",
    "train_incrementally(dt, m_train_X, m_train_y)\n",
    "test_incrementally(dt, m_test_X, m_test_y), dt.get_n_leaves(), dt.get_depth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0747edaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "efdt = tree.ExtremelyFastDecisionTreeClassifier(max_depth=5)\n",
    "train_incrementally(efdt, m_train_X, list(m_train_y))\n",
    "test_incrementally(efdt, m_test_X, list(m_test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "96be391f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 5)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "efdt.n_leaves,efdt.max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "be9dd2ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6003161118245579"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "efdt = tree.ExtremelyFastDecisionTreeClassifier(max_depth=5)\n",
    "train_incrementally(efdt, m_train_X, list(m_train_y))\n",
    "test_incrementally(efdt, m_test_X, list(m_test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0002db8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "      <th>lepton_pT</th>\n",
       "      <th>lepton_eta</th>\n",
       "      <th>lepton_phi</th>\n",
       "      <th>missing_energy_magnitude</th>\n",
       "      <th>missing_energy_phi</th>\n",
       "      <th>jet_1_pt</th>\n",
       "      <th>jet_1_eta</th>\n",
       "      <th>jet_1_phi</th>\n",
       "      <th>jet_1_b-tag</th>\n",
       "      <th>...</th>\n",
       "      <th>jet_4_eta</th>\n",
       "      <th>jet_4_phi</th>\n",
       "      <th>jet_4_b-tag</th>\n",
       "      <th>m_jj</th>\n",
       "      <th>m_jjj</th>\n",
       "      <th>m_lv</th>\n",
       "      <th>m_jlv</th>\n",
       "      <th>m_bb</th>\n",
       "      <th>m_wbb</th>\n",
       "      <th>m_wwbb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.869293</td>\n",
       "      <td>-0.635082</td>\n",
       "      <td>0.225690</td>\n",
       "      <td>0.327470</td>\n",
       "      <td>-0.689993</td>\n",
       "      <td>0.754202</td>\n",
       "      <td>-0.248573</td>\n",
       "      <td>-1.092064</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010455</td>\n",
       "      <td>-0.045767</td>\n",
       "      <td>3.101961</td>\n",
       "      <td>1.353760</td>\n",
       "      <td>0.979563</td>\n",
       "      <td>0.978076</td>\n",
       "      <td>0.920005</td>\n",
       "      <td>0.721657</td>\n",
       "      <td>0.988751</td>\n",
       "      <td>0.876678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.907542</td>\n",
       "      <td>0.329147</td>\n",
       "      <td>0.359412</td>\n",
       "      <td>1.497970</td>\n",
       "      <td>-0.313010</td>\n",
       "      <td>1.095531</td>\n",
       "      <td>-0.557525</td>\n",
       "      <td>-1.588230</td>\n",
       "      <td>2.173076</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.138930</td>\n",
       "      <td>-0.000819</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.302220</td>\n",
       "      <td>0.833048</td>\n",
       "      <td>0.985700</td>\n",
       "      <td>0.978098</td>\n",
       "      <td>0.779732</td>\n",
       "      <td>0.992356</td>\n",
       "      <td>0.798343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.798835</td>\n",
       "      <td>1.470639</td>\n",
       "      <td>-1.635975</td>\n",
       "      <td>0.453773</td>\n",
       "      <td>0.425629</td>\n",
       "      <td>1.104875</td>\n",
       "      <td>1.282322</td>\n",
       "      <td>1.381664</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.128848</td>\n",
       "      <td>0.900461</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.909753</td>\n",
       "      <td>1.108330</td>\n",
       "      <td>0.985692</td>\n",
       "      <td>0.951331</td>\n",
       "      <td>0.803252</td>\n",
       "      <td>0.865924</td>\n",
       "      <td>0.780118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.344385</td>\n",
       "      <td>-0.876626</td>\n",
       "      <td>0.935913</td>\n",
       "      <td>1.992050</td>\n",
       "      <td>0.882454</td>\n",
       "      <td>1.786066</td>\n",
       "      <td>-1.646778</td>\n",
       "      <td>-0.942383</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.678379</td>\n",
       "      <td>-1.360356</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.946652</td>\n",
       "      <td>1.028704</td>\n",
       "      <td>0.998656</td>\n",
       "      <td>0.728281</td>\n",
       "      <td>0.869200</td>\n",
       "      <td>1.026736</td>\n",
       "      <td>0.957904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.105009</td>\n",
       "      <td>0.321356</td>\n",
       "      <td>1.522401</td>\n",
       "      <td>0.882808</td>\n",
       "      <td>-1.205349</td>\n",
       "      <td>0.681466</td>\n",
       "      <td>-1.070464</td>\n",
       "      <td>-0.921871</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.373566</td>\n",
       "      <td>0.113041</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.755856</td>\n",
       "      <td>1.361057</td>\n",
       "      <td>0.986610</td>\n",
       "      <td>0.838085</td>\n",
       "      <td>1.133295</td>\n",
       "      <td>0.872245</td>\n",
       "      <td>0.808487</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   prediction  lepton_pT  lepton_eta  lepton_phi  missing_energy_magnitude  \\\n",
       "0         1.0   0.869293   -0.635082    0.225690                  0.327470   \n",
       "1         1.0   0.907542    0.329147    0.359412                  1.497970   \n",
       "2         1.0   0.798835    1.470639   -1.635975                  0.453773   \n",
       "3         0.0   1.344385   -0.876626    0.935913                  1.992050   \n",
       "4         1.0   1.105009    0.321356    1.522401                  0.882808   \n",
       "\n",
       "   missing_energy_phi  jet_1_pt  jet_1_eta  jet_1_phi  jet_1_b-tag  ...  \\\n",
       "0           -0.689993  0.754202  -0.248573  -1.092064     0.000000  ...   \n",
       "1           -0.313010  1.095531  -0.557525  -1.588230     2.173076  ...   \n",
       "2            0.425629  1.104875   1.282322   1.381664     0.000000  ...   \n",
       "3            0.882454  1.786066  -1.646778  -0.942383     0.000000  ...   \n",
       "4           -1.205349  0.681466  -1.070464  -0.921871     0.000000  ...   \n",
       "\n",
       "   jet_4_eta  jet_4_phi  jet_4_b-tag      m_jj     m_jjj      m_lv     m_jlv  \\\n",
       "0  -0.010455  -0.045767     3.101961  1.353760  0.979563  0.978076  0.920005   \n",
       "1  -1.138930  -0.000819     0.000000  0.302220  0.833048  0.985700  0.978098   \n",
       "2   1.128848   0.900461     0.000000  0.909753  1.108330  0.985692  0.951331   \n",
       "3  -0.678379  -1.360356     0.000000  0.946652  1.028704  0.998656  0.728281   \n",
       "4  -0.373566   0.113041     0.000000  0.755856  1.361057  0.986610  0.838085   \n",
       "\n",
       "       m_bb     m_wbb    m_wwbb  \n",
       "0  0.721657  0.988751  0.876678  \n",
       "1  0.779732  0.992356  0.798343  \n",
       "2  0.803252  0.865924  0.780118  \n",
       "3  0.869200  1.026736  0.957904  \n",
       "4  1.133295  0.872245  0.808487  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Higgs\n",
    "df_higgs = pd.read_csv(\"/Users/scott/projects/research-projects/tree_diff/input/Higgs/HIGGS.csv\", header=None)\n",
    "df_higgs.columns = [\"prediction\",\"lepton_pT\",\"lepton_eta\",\"lepton_phi\",\"missing_energy_magnitude\",\"missing_energy_phi\",\"jet_1_pt\",\"jet_1_eta\",\"jet_1_phi\",\"jet_1_b-tag\",\"jet_2_pt\",\"jet_2_eta\",\"jet_2_phi\",\"jet_2_b-tag\",\"jet_3_pt\",\"jet_3_eta\",\"jet_3_phi\",\"jet_3_b-tag\",\"jet_4_pt\",\"jet_4_eta\",\"jet_4_phi\",\"jet_4_b-tag\",\"m_jj\",\"m_jjj\",\"m_lv\",\"m_jlv\",\"m_bb\",\"m_wbb\",\"m_wwbb\"]\n",
    "df_higgs.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9d8fd31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6649874736169565"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "higgs_features = [\"lepton_pT\",\"lepton_eta\",\"lepton_phi\",\"missing_energy_magnitude\",\"missing_energy_phi\",\"jet_1_pt\",\"jet_1_eta\",\"jet_1_phi\",\"jet_1_b-tag\",\"jet_2_pt\",\"jet_2_eta\",\"jet_2_phi\",\"jet_2_b-tag\",\"jet_3_pt\",\"jet_3_eta\",\"jet_3_phi\",\"jet_3_b-tag\",\"jet_4_pt\",\"jet_4_eta\",\"jet_4_phi\",\"jet_4_b-tag\",\"m_jj\",\"m_jjj\",\"m_lv\",\"m_jlv\",\"m_bb\",\"m_wbb\",\"m_wwbb\"]\n",
    "dt = DecisionTreeClassifier(max_depth=100)\n",
    "\n",
    "h_tr_X, h_te_X, h_tr_y, h_te_y = sklearn.model_selection.train_test_split(df_higgs[higgs_features], df_higgs[\"prediction\"], test_size=0.3)\n",
    "\n",
    "dt.fit(h_tr_X, h_tr_y)\n",
    "y_pred = dt.predict(h_te_X)\n",
    "balanced_accuracy_score(h_te_y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7604048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# node_depths = [1,2,5,10,15,20,25]\n",
    "# for i in node_depths:\n",
    "#     efdt = tree.ExtremelyFastDecisionTreeClassifier(max_depth=i)  \n",
    "#     train_incrementally(efdt, android_train_X, android_train_y)\n",
    "#     efdt_score = test_incrementally(efdt, android_test_X, android_test_y)    \n",
    "    \n",
    "#     df = DecisionTreeClassifier(max_depth=i)\n",
    "#     train_incrementally(dt, android_train_X, android_train_y)\n",
    "#     dt_score = test_incrementally(dt, android_test_X, android_test_y)    \n",
    "    \n",
    "#     efdt_score = test_incrementally(efdt, android_test_X, android_test_y)\n",
    "#     print(f\"Max nodes: {i}, DT: {dt_score:0.2f}, EFDT: {efdt_score:0.2f}\")  \n",
    "\n",
    "\n",
    "def walk_tree(node, fetch_children, is_leaf):\n",
    "    \"\"\"Generator function that walks a tree.\n",
    "\n",
    "    fetch_children: Callable that accepts a node and path_to_node.\n",
    "        Returns a List[Tuple] where Tuple = (path_to_node, child_node)\n",
    "\n",
    "    is_leaf: Callable that accepts a node and returns true if a leaf node.\n",
    "\n",
    "    Usage:\n",
    "    >>> [p for p in walk_tree(tree, lambda x, p: [(p, x.left), (p, x.right)], lambda x: x.is_leaf)]\n",
    "    \"\"\"\n",
    "    stack = deque()\n",
    "    stack.append(([],node))\n",
    "    while stack:\n",
    "        path_to_node, node = stack.pop()\n",
    "        if is_leaf(node):\n",
    "            yield path_to_node + [node]\n",
    "        else:\n",
    "            children = fetch_children(node, path_to_node)\n",
    "            for child in children:\n",
    "                stack.append(child)   \n",
    "\n",
    "def walk_decision_node(root):                \n",
    "    for p in walk_tree(root, \n",
    "                       lambda n,p: [(p+[n], i) for i in n.children], \n",
    "                       lambda x: x.children == []):\n",
    "        yield p\n",
    "\n",
    "def print_tree(root):\n",
    "    for i in walk_decision_node(root):\n",
    "        print(root)\n",
    "\n",
    "def predict(root, x):\n",
    "    for i in walk_decision_node(root):\n",
    "        if i.predict(x):\n",
    "            return i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a6ffb6",
   "metadata": {},
   "source": [
    "# Experiment\n",
    "* No missing values\n",
    "* Range of samples\n",
    "* Mixed attributes\n",
    "\n",
    "## Datasets\n",
    "* Census Income Data (Mixed)\n",
    "* Mushroom Dataset(mostly categorical)\n",
    "* Android dataset (sparse)\n",
    "* HIGGS (numerical)\n",
    "\n",
    "## Benchmarks\n",
    "* Tree: EFDT\n",
    "* Tree: HT\n",
    "* Tree: DT\n",
    "* Interpretable: FIGS\n",
    "* Interpretable: RuleCosi+\n",
    "\n",
    "## Notes\n",
    "* Hard > Soft\n",
    "* < Oblique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cbef29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
